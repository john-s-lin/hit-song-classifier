{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hit-song-classifier\n",
    "\n",
    "## Multiclass Logistic Regression\n",
    "\n",
    "The first thing we'll do is define our imports and declare any constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Define our constants\n",
    "TARGET_DATA = \"../../data/spotify_enhanced_dataset.csv\"\n",
    "RANDOM_SEED = 0\n",
    "TARGET_FEATURES = [\n",
    "    \"popularity\",\n",
    "    \"danceability\",\n",
    "    \"energy\",\n",
    "    \"key\",\n",
    "    \"loudness\",\n",
    "    \"mode\",\n",
    "    \"speechiness\",\n",
    "    \"acousticness\",\n",
    "    \"instrumentalness\",\n",
    "    \"liveness\",\n",
    "    \"valence\",\n",
    "    \"tempo\",\n",
    "    \"duration_ms\",\n",
    "    \"time_signature\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall split our data in a 60-20-20 train_test_val split. Then we shall build our set of inputs X. We don't want to include the features `[song, artist, year]` since they are keys. We also should define our label `class` since that is our classification feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19516\n",
      "Test: 6527\n",
      "Val: 6466\n"
     ]
    }
   ],
   "source": [
    "def feature_transformer(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Assembles feature vectors in dataframe\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): input\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: feature-label dataframe\n",
    "    \"\"\"\n",
    "    va = VectorAssembler(inputCols=TARGET_FEATURES, outputCol=\"features\")\n",
    "    va_df = va.transform(df)\n",
    "    return va_df\n",
    "\n",
    "\n",
    "def load_data(filename: str) -> DataFrame:\n",
    "    \"\"\"Load data from file\n",
    "\n",
    "    Args:\n",
    "        filename (str): filename\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: pyspark dataframe\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName(\"hit-song-classifier-rfc\").getOrCreate()\n",
    "    return spark.read.csv(filename, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "def split_data(df: DataFrame) -> tuple[DataFrame, DataFrame, DataFrame]:\n",
    "    \"\"\"Splits data to train, val, test splits\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): full dataframe\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: _description_\n",
    "    \"\"\"\n",
    "    train, val, test = df.randomSplit([0.6, 0.2, 0.2], seed=RANDOM_SEED)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "df = feature_transformer(load_data(TARGET_DATA))\n",
    "train_df, val_df, test_df = split_data(df)\n",
    "print(f\"Train: {train_df.count()}\\nTest: {test_df.count()}\\nVal: {val_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search: training multiple estimators\n",
    "\n",
    "Here we will train multiple estimators with different hyperparameter settings. For Logistic Regression, the hyperparameters we've chosen include `regParam` and `maxIter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimators(\n",
    "    data: DataFrame,\n",
    "    estimator_type: any,\n",
    "    param_name: str,\n",
    "    param_vals: list[int],\n",
    "    **kwargs,\n",
    ") -> list:\n",
    "    \"\"\"Trains estimators\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): dataframe\n",
    "        estimator_type (RandomForestClassifier): estimator\n",
    "        param_name (str): parameter name\n",
    "        param_vals (list[int]): parameter values\n",
    "\n",
    "    Returns:\n",
    "        list: list of trained estimators\n",
    "    \"\"\"\n",
    "    estimators = []\n",
    "    for val in param_vals:\n",
    "        estimator = estimator_type(**{param_name: val}, **kwargs)\n",
    "        one_vs_rest = OneVsRest(classifier=estimator, **kwargs)\n",
    "        fitted_models = one_vs_rest.fit(data)\n",
    "        estimators.append(fitted_models)\n",
    "        print(f\"Training {fitted_models}\")\n",
    "    return estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search: `regParam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training OneVsRestModel_cbd82b0270d8\n",
      "Training OneVsRestModel_0181fdcb68fa\n",
      "Training OneVsRestModel_98b47e292f4c\n"
     ]
    }
   ],
   "source": [
    "reg_params = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1]\n",
    "lr_reg_params_list = train_estimators(\n",
    "    train_df,\n",
    "    LinearSVC,\n",
    "    \"regParam\",\n",
    "    reg_params,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot estimator scores to pick the estimator with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def score_model(models: list, data: DataFrame) -> list:\n",
    "    \"\"\"Scores models\n",
    "\n",
    "    Args:\n",
    "        estimators (list): list of estimators\n",
    "        data (DataFrame): dataframe\n",
    "\n",
    "    Returns:\n",
    "        list: list of scores\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"class\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    predicted = [model.transform(data) for model in models]\n",
    "    return [evaluator.evaluate(pred) for pred in predicted]\n",
    "\n",
    "\n",
    "print(f\"train: {score_model(lr_reg_params_list, train_df)}\")\n",
    "print(f\"val: {score_model(lr_reg_params_list, val_df)}\")\n",
    "print(f\"test: {score_model(lr_reg_params_list, test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter search: `maxIter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "max_iter = [1, 5, 10, 20, 50, 100]\n",
    "lr_max_iter_list = train_estimators(\n",
    "    train_df,\n",
    "    LinearSVC,\n",
    "    \"maxIter\",\n",
    "    max_iter,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"class\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the F1 score of the hyperparameters for `regParam` and `maxIter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"train: {score_model(lr_max_iter_list, train_df)}\")\n",
    "print(f\"val: {score_model(lr_max_iter_list, val_df)}\")\n",
    "print(f\"test: {score_model(lr_max_iter_list, test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_estimator_scores(models: list, param_name: str, param_vals: list[int]):\n",
    "    \"\"\"Plots the scores of a list of estimators\"\"\"\n",
    "\n",
    "    train_score = score_model(models, train_df)\n",
    "    val_score = score_model(models, val_df)\n",
    "    test_score = score_model(models, test_df)\n",
    "\n",
    "    # Get the best validation score and the index of the best estimator\n",
    "    best_val_score = max(val_score)\n",
    "    best_val_idx = val_score.index(best_val_score)\n",
    "\n",
    "    # Plot metrics\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(train_score)), train_score, \"go-\", label=\"train\")\n",
    "    plt.plot(np.arange(len(val_score)), val_score, \"ro-\", label=\"val\")\n",
    "    plt.plot(np.arange(len(test_score)), test_score, \"k:\", label=\"test\")\n",
    "    # Place an X at the best validation score\n",
    "    plt.scatter(best_val_idx, best_val_score, marker=\"x\", color=\"r\", s=200)\n",
    "\n",
    "    # Additional formatting\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xticks(ticks=np.arange(len(train_score)), labels=param_vals)\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.title(f\"{models[0].__class__.__name__} score vs. {param_name}\")\n",
    "\n",
    "    # Output scores at max_depth of the best_validation_score\n",
    "    plt.text(\n",
    "        3.7 if len(train_score) < 6 else 4.7,\n",
    "        0.3,\n",
    "        f\"train = {train_score[best_val_idx]:.3f}\",\n",
    "        c=\"g\",\n",
    "        ha=\"right\",\n",
    "    )\n",
    "    plt.text(\n",
    "        3.7 if len(train_score) < 6 else 4.7,\n",
    "        0.2,\n",
    "        f\"validate = {best_val_score:.3f}\",\n",
    "        c=\"r\",\n",
    "        ha=\"right\",\n",
    "    )\n",
    "    plt.text(\n",
    "        3.7 if len(train_score) < 6 else 4.7,\n",
    "        0.1,\n",
    "        f\"test = {test_score[best_val_idx]:.3f}\",\n",
    "        c=\"k\",\n",
    "        ha=\"right\",\n",
    "    )\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_estimator_scores(lr_reg_params_list, \"regParam\", reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_estimator_scores(lr_max_iter_list, \"maxIter\", max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation using Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Declare models and model names\n",
    "model_list = [lr_reg_params_list, lr_max_iter_list]\n",
    "model_names = [\"OneVsRest SVM Regularization\", \"OneVsRest SVM Max Iterations\"]\n",
    "\n",
    "\n",
    "def generate_confusion_matrix(model_list: list, val_df: DataFrame, test_df: DataFrame):\n",
    "    \"\"\"Generates a confusion matrix for each model in model_list\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    for i, model in enumerate(model_list):\n",
    "        val_score = score_model(model, val_df)\n",
    "        best_val_score = max(val_score)\n",
    "        best_val_idx = val_score.index(best_val_score)\n",
    "\n",
    "        # Generate confusion matrix\n",
    "        best_model = (\n",
    "            model[best_val_idx].transform(test_df).select(\"class\", \"prediction\")\n",
    "        )\n",
    "        predictionAndLabels = best_model.rdd.map(\n",
    "            lambda row: (float(row[\"prediction\"]), float(row[\"class\"]))\n",
    "        )\n",
    "        class_names = (\n",
    "            best_model.select(\"class\")\n",
    "            .distinct()\n",
    "            .sort(\"class\")\n",
    "            .rdd.map(lambda row: row[0])\n",
    "            .collect()\n",
    "        )\n",
    "        metrics = MulticlassMetrics(predictionAndLabels)\n",
    "        con_mtrx = metrics.confusionMatrix().toArray().astype(int)\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=con_mtrx, display_labels=class_names\n",
    "        )\n",
    "        disp.plot(ax=axes[i], colorbar=False)\n",
    "        axes[i % 2].set_title(model_names[i % 2])\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(disp.im_, ax=axes.ravel().tolist())\n",
    "\n",
    "\n",
    "generate_confusion_matrix(model_list, val_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together using Cross-Validation\n",
    "\n",
    "Since we evaluated the hyperparameters separately above, we should create a Pipeline in which the best hyperparameters are determined grid-wise. This should automate what we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcross_validator\u001B[39m(train_data: \u001B[43mDataFrame\u001B[49m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m CrossValidatorModel:\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124;03m\"\"\"Performs cross validation\"\"\"\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# Define the estimator\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "def cross_validator(train_data: DataFrame) -> CrossValidatorModel:\n",
    "    \"\"\"Performs cross validation\"\"\"\n",
    "\n",
    "    # Define the estimator\n",
    "    lr = LinearSVC()\n",
    "    one_vs_rest = OneVsRest(classifier=lr)\n",
    "\n",
    "    # Define the parameter grid\n",
    "    param_grid = (\n",
    "        ParamGridBuilder()\n",
    "        .addGrid(lr.regParam, reg_params)\n",
    "        .addGrid(lr.maxIter, max_iter)\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # Define the evaluator using F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"class\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "\n",
    "    # Create the cross validator with 5 folds\n",
    "    cv = CrossValidator(\n",
    "        estimator=one_vs_rest, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3\n",
    "    )\n",
    "    cv_model = cv.fit(train_data)\n",
    "\n",
    "    return cv_model\n",
    "\n",
    "\n",
    "cv_model = cross_validator(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cross_val_confusion_matrix(\n",
    "    cv_model: CrossValidatorModel, test_df: DataFrame\n",
    "):\n",
    "    \"\"\"Generates a confusion matrix for the best model from cross validation\"\"\"\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    predictionAndLabels = (\n",
    "        cv_model.transform(test_df)\n",
    "        .select(\"class\", \"prediction\")\n",
    "        .rdd.map(lambda row: (float(row[\"prediction\"]), float(row[\"class\"])))\n",
    "    )\n",
    "    class_names = (\n",
    "        test_df.select(\"class\")\n",
    "        .distinct()\n",
    "        .sort(\"class\")\n",
    "        .rdd.map(lambda row: row[0])\n",
    "        .collect()\n",
    "    )\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    con_mtrx = metrics.confusionMatrix().toArray().astype(int)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=con_mtrx, display_labels=class_names)\n",
    "    disp.plot()\n",
    "    plt.title(\"Cross Validation Confusion Matrix\")\n",
    "\n",
    "\n",
    "generate_cross_val_confusion_matrix(cv_model, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining best hyperparameters\n",
    "\n",
    "Now we should explain the best hyperparameters for the cross-validation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_model.bestModel.explainParam(\"regParam\"))\n",
    "print(cv_model.bestModel.explainParam(\"maxIter\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
